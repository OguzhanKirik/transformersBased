Vision Transformer & Transformer (From Scratch)

This repository contains from-scratch implementations of the original Transformer and Vision Transformer (ViT) architectures, built without high-level model libraries.

The goal is educational clarity: every core component (attention, positional encoding, patch embedding, encoder blocks) is implemented explicitly to mirror the original papers.

Implemented

Transformer (Encoder / Encoderâ€“Decoder)

Multi-Head Self-Attention

Positional Encoding

Vision Transformer (patch embedding + Transformer encoder)

Purpose

Understand how Transformers and ViTs work under the hood, not just how to use them.
