{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a1cd8a",
   "metadata": {},
   "source": [
    "## Transfomrer Achtiecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2ed7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Positional Embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_length, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.dropout(X + self.pos_embed[:X.size(1)])\n",
    "    \n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim,  num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = num_heads\n",
    "        self.d = embed_dim // num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1,2)\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        q = self.split_heads(self.q_proj(query)) # (B,h, Lq, d)\n",
    "        k = self.split_heads(self.k_proj(key)) # (B, h, Lk, d)\n",
    "        v = self.split_heads(self.v_proj(value)) #(B, h, Lv,d) with Lv=Lk \n",
    "        scores = q @ k.transpose(2,3) / self.d**0.5 # (B, h, Lq, Lk)\n",
    "        \n",
    "        #Masking support\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask, -torch.inf) # (B,h, Lq, Lk)\n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n",
    "            scores = scores.masked_fill(mask, -torch.inf)  # (B, h, Lq, Lk)\n",
    "        \n",
    "        weigths = scores.softmax(dim=-1) # (B,h, Lq, Lk)\n",
    "        Z = self.dropout(weigths) @ v # (B,h,Lq, d)\n",
    "        Z = Z.transpose(1,2) # (B, Lq, h,d)\n",
    "        Z = Z.reshape(Z.size(0), Z.size(1), self.h*self.d) # B, Lq, h x d \n",
    "        return (self.out_proj(Z), weigths)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fa04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropput = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                                 key_padding_mask=src_key_padding_mask)\n",
    "        Z = self.norm1(src + self.dropout(attn))\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        return self.norm2(Z + ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d549bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.dropput = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        attn1, _ = self.self_attn(tgt, tgt, tgt,\n",
    "                                  attn_mask=tgt_mask,\n",
    "                                  key_padding_mask=tgt_key_padding_mask)\n",
    "        Z = self.norm1(tgt + self.dropout(attn1))\n",
    "        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask,\n",
    "                                       key_padding_mask=memory_key_padding_mask)\n",
    "        Z = self.norm2(Z + self.dropout(attn2))\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        return self.norm3(Z + ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d685ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(encoder_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        Z = src\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, mask, src_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243a4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(decode_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        Z = tgt\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, memory, tgt_mask, memory_mask,\n",
    "                      tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfb78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm1 = nn.LayerNorm(d_model)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
    "                                          norm1)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm2 = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers,\n",
    "                                          norm2)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n",
    "                              tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d2acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02150a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd35855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353161c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
