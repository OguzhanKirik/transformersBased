{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a1cd8a",
   "metadata": {},
   "source": [
    "## Transformer Achtiecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13bbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Positional Embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Adds learnable positional information to embeddings\"\"\"\n",
    "    def __init__(self, max_length, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Learnable positional embeddings (max_length x embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Add positional encoding to input embeddings, slice to match sequence length\n",
    "        return self.dropout(X + self.pos_embed[:X.size(1)])\n",
    "    \n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = num_heads  # Number of attention heads\n",
    "        self.d = embed_dim // num_heads  # Dimension per head\n",
    "        # Linear projections for queries, keys, values\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # Output projection to combine heads\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        \"\"\"Split embedding dimension into multiple heads\"\"\"\n",
    "        # (B, L, embed_dim) -> (B, L, h, d) -> (B, h, L, d)\n",
    "        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # Project and split into heads\n",
    "        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n",
    "        k = self.split_heads(self.k_proj(key))    # (B, h, Lk, d)\n",
    "        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) where Lv=Lk\n",
    "        \n",
    "        # Compute attention scores: Q * K^T / sqrt(d)\n",
    "        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n",
    "        \n",
    "        # Apply attention mask (for causal/future masking)\n",
    "        if attn_mask is not None:\n",
    "            # Set masked positions to -inf (will become 0 after softmax)\n",
    "            scores = scores.masked_fill(attn_mask, -torch.inf)\n",
    "        \n",
    "        # Apply padding mask (ignore padding tokens)\n",
    "        if key_padding_mask is not None:\n",
    "            # Expand mask dimensions to match scores shape\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n",
    "            scores = scores.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        # Compute attention weights via softmax\n",
    "        weights = scores.softmax(dim=-1)  # (B, h, Lq, Lk)\n",
    "        \n",
    "        # Apply dropout and multiply by values\n",
    "        Z = self.dropout(weights) @ v  # (B, h, Lq, d)\n",
    "        \n",
    "        # Merge heads back together\n",
    "        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n",
    "        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, embed_dim)\n",
    "        \n",
    "        # Final output projection and return with attention weights\n",
    "        return (self.out_proj(Z), weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af35f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Single layer of transformer encoder (self-attention + feedforward)\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        # Two-layer feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        # Layer normalization (applied after residual connections)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self-attention block with residual connection\n",
    "        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                                 key_padding_mask=src_key_padding_mask)\n",
    "        # Add & Norm: residual connection + layer normalization\n",
    "        Z = self.norm1(src + self.dropout(attn))\n",
    "        \n",
    "        # Feedforward block: linear -> ReLU -> dropout -> linear -> dropout\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        # Add & Norm: residual connection + layer normalization\n",
    "        return self.norm2(Z + ff)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"Single layer of transformer decoder (masked self-attention + cross-attention + feedforward)\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Masked self-attention (decoder attends to previous tokens)\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        # Cross-attention (decoder attends to encoder output)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Two-layer feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        # Three layer norms (one for each sub-layer)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Masked self-attention (target attends to itself with causal mask)\n",
    "        attn1, _ = self.self_attn(tgt, tgt, tgt,\n",
    "                                  attn_mask=tgt_mask,\n",
    "                                  key_padding_mask=tgt_key_padding_mask)\n",
    "        # Add & Norm\n",
    "        Z = self.norm1(tgt + self.dropout(attn1))\n",
    "        \n",
    "        # Cross-attention (decoder queries encoder's memory)\n",
    "        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask,\n",
    "                                       key_padding_mask=memory_key_padding_mask)\n",
    "        # Add & Norm\n",
    "        Z = self.norm2(Z + self.dropout(attn2))\n",
    "        \n",
    "        # Feedforward block\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        # Add & Norm\n",
    "        return self.norm3(Z + ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ed7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Stack of N encoder layers\"\"\"\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        # Create N independent copies of the encoder layer\n",
    "        self.layers = nn.ModuleList([deepcopy(encoder_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        # Optional final layer normalization\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        Z = src  # Start with input\n",
    "        # Pass through each encoder layer sequentially\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, mask, src_key_padding_mask)\n",
    "        # Apply final normalization if provided\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Stack of N decoder layers\"\"\"\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        # Create N independent copies of the decoder layer\n",
    "        self.layers = nn.ModuleList([deepcopy(decoder_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        # Optional final layer normalization\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        Z = tgt  # Start with target input\n",
    "        # Pass through each decoder layer sequentially\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, memory, tgt_mask, memory_mask,\n",
    "                      tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        # Apply final normalization if provided\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete transformer model (encoder + decoder)\"\"\"\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Create single encoder layer template\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm1 = nn.LayerNorm(d_model)\n",
    "        # Build encoder stack (6 layers by default)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
    "                                          norm1)\n",
    "        \n",
    "        # Create single decoder layer template\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm2 = nn.LayerNorm(d_model)\n",
    "        # Build decoder stack (6 layers by default)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers,\n",
    "                                          norm2)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        # Encode source sequence into memory\n",
    "        memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
    "        # Decode target sequence using encoder's memory\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n",
    "                              tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d966413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
